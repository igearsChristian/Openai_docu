<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Generation Guide</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f9f9f9;
            color: #333;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        code {
            background-color: #f0f0f0;
            padding: 2px 4px;
            border-radius: 4px;
        }
        pre {
            background-color: #f0f0f0;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
        ul {
            margin: 10px 0;
        }
        .quickstart {
            background-color: #e9f5ff;
            border-left: 5px solid #2196F3;
            padding: 10px;
            margin: 20px 0;
        }
    </style>
</head>
<body>

<h1>Audio Generation</h1>

<p>Learn how to generate audio from a text or audio prompt.</p>

<p>In addition to generating <a href="/docs/guides/text-generation">text</a> and <a href="/docs/guides/images">images</a>, some <a href="/docs/models">models</a> enable you to generate a spoken audio response to a prompt, and to use audio inputs to prompt the model. Audio inputs can contain richer data than text alone, allowing the model to detect tone, inflection, and other nuances within the input.</p>

<p>You can use these audio capabilities to:</p>
<ul>
    <li>Generate a spoken audio summary of a body of text (text in, audio out)</li>
    <li>Perform sentiment analysis on a recording (audio in, text out)</li>
    <li>Async speech to speech interactions with a model (audio in, audio out)</li>
</ul>

<p>OpenAI provides other models for simple <a href="/docs/guides/speech-to-text">speech to text</a> and <a href="/docs/guides/text-to-speech">text to speech</a> - when your task requires those conversions (and not dynamic content from a model), the TTS and STT models will be more performant and cost-efficient.</p>

<h2>Quickstart</h2>

<p>To generate audio or use audio as an input, you can use the <a href="/docs/api-reference/chat/">chat completions endpoint</a> in the REST API, as seen in the examples below. You can either use the <a href="/docs/api-reference">REST API</a> from the HTTP client of your choice, or use one of OpenAI's <a href="/docs/libraries">official SDKs</a> for your preferred programming language.</p>

    <h3>Audio output from model</h3>
    <p>Create a human-like audio response to a prompt</p>

    <pre><code>import { writeFileSync } from "node:fs";
import OpenAI from "openai";

const openai = new OpenAI();

// Generate an audio response to the given prompt
const response = await openai.chat.completions.create({
  model: "gpt-4o-audio-preview",
  modalities: ["text", "audio"],
  audio: { voice: "alloy", format: "wav" },
  messages: [
    {
      role: "user",
      content: "Is a golden retriever a good family dog?"
    }
  ]
});

// Inspect returned data
console.log(response.choices[0]);

// Write audio data to a file
writeFileSync(
  "dog.wav",
  Buffer.from(response.choices[0].message.audio.data, 'base64'),
  { encoding: "utf-8" }
);</code></pre>

    <pre><code>import base64
from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o-audio-preview",
    modalities=["text", "audio"],
    audio={"voice": "alloy", "format": "wav"},
    messages=[
        {
            "role": "user",
            "content": "Is a golden retriever a good family dog?"
        }
    ]
)

print(completion.choices[0])

wav_bytes = base64.b64decode(completion.choices[0].message.audio.data)
with open("dog.wav", "wb") as f:
    f.write(wav_bytes)</code></pre>

    <pre><code>curl "https://api.openai.com/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
      "model": "gpt-4o-audio-preview",
      "modalities": ["text", "audio"],
      "audio": { "voice": "alloy", "format": "wav" },
      "messages": [
        {
          "role": "user",
          "content": "Is a golden retriever a good family dog?"
        }
      ]
    }'</code></pre>



    <h3>Audio input to model</h3>
    <p>Use audio inputs for prompting a model</p>

    <pre><code>import OpenAI from "openai";
const openai = new OpenAI();

// Fetch an audio file and convert it to a base64 string
const url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav";
const audioResponse = await fetch(url);
const buffer = await audioResponse.arrayBuffer();
const base64str = Buffer.from(buffer).toString("base64");

const response = await openai.chat.completions.create({
  model: "gpt-4o-audio-preview",
  modalities: ["text", "audio"],
  audio: { voice: "alloy", format: "wav" },
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "What is in this recording?" },
        { type: "input_audio", input_audio: { data: base64str, format: "wav" }}
      ]
    }
  ]
});

console.log(response.choices[0]);</code></pre>

    <pre><code>import base64
import requests
from openai import OpenAI

client = OpenAI()

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content
encoded_string = base64.b64encode(wav_data).decode('utf-8')

completion = client.chat.completions.create(
    model="gpt-4o-audio-preview",
    modalities=["text", "audio"],
    audio={"voice": "alloy", "format": "wav"},
    messages=[
        {
            "role": "user",
            "content": [
                { 
                    "type": "text",
                    "text": "What is in this recording?"
                },
                {
                    "type": "input_audio",
                    "input_audio": {
                        "data": encoded_string,
                        "format": "wav"
                    }
                }
            ]
        },
    ]
)

print(completion.choices[0].message)</code></pre>

    <pre><code>curl "https://api.openai.com/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
      "model": "gpt-4o-audio-preview",
      "modalities": ["text", "audio"],
      "audio": { "voice": "alloy", "format": "wav" },
      "messages": [
        {
          "role": "user",
          "content": [
            { "type": "text", "text": "What is in this recording?" },
            { 
              "type": "input_audio", 
              "input_audio": { 
                "data": "<base64 bytes here>", 
                "format": "wav" 
              }
            }
          ]
        }
      ]
    }'</code></pre>


<h2>Multi-turn conversations</h2>

<p>Using audio outputs from the model as inputs to multi-turn conversations requires a generated ID that appears in the response data for an audio generation. Below is an example JSON data structure for a <a href="/docs/api-reference/chat/object#chat/object-choices">message you might receive</a> from <code>/chat/completions</code>:</p>

<pre><code>{
  "index": 0,
  "message": {
    "role": "assistant",
    "content": null,
    "refusal": null,
    "audio": {
      "id": "audio_abc123",
      "expires_at": 1729018505,
      "data": "&lt;bytes omitted&gt;",
      "transcript": "Yes, golden retrievers are known to be ..."
    }
  },
  "finish_reason": "stop"
}</code></pre>

<p>The value of <code>message.audio.id</code> above provides an identifier you can use in an <code>assistant</code> message for a new <code>/chat/completions</code> request, as in the example below.</p>

<pre><code>curl "https://api.openai.com/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
        "model": "gpt-4o-audio-preview",
        "modalities": ["text", "audio"],
        "audio": { "voice": "alloy", "format": "wav" },
        "messages": [
            {
                "role": "user",
                "content": "Is a golden retriever a good family dog?"
            },
            {
                "role": "assistant",
                "audio": {
                    "id": "audio_abc123"
                }
            },
            {
                "role": "user",
                "content": "Why do you say they are loyal?"
            }
        ]
    }'</code></pre>

<h2>FAQ</h2>

<h3>What modalities are supported by gpt-4o-audio-preview</h3>
<p><code>gpt-4o-audio-preview</code> requires either audio output or audio input to be used at this time. Acceptable combinations of input and output are:</p>
<ul>
    <li>text in → text + audio out</li>
    <li>audio in → text + audio out</li>
    <li>audio in → text out</li>
    <li>text + audio in → text + audio out</li>
    <li>text + audio in → text out</li>
</ul>

<h3>How is audio in Chat Completions different from the Realtime API?</h3>
<p>The underlying GPT-4o audio model is exactly the same. The Realtime API operates the same model at lower latency.</p>

<h3>How do I think about audio input to the model in terms of tokens?</h3>
<p>We are working on better tooling to expose this, but roughly one hour of audio input is equal to 128k tokens, the max context window currently supported by this model.</p>

<h3>How do I control which output modalities I receive?</h3>
<p>Currently the model only programmatically allows modalities = <code>["text", "audio"]</code>. In the future, this parameter will give more controls.</p>

<h3>How does tool/function calling work?</h3>
<p>Tool (and function) calling works the same as it does for other models in Chat Completions - <a href="/docs/guides/function-calling">learn more</a>.</p>

<h2>Next steps</h2>

<p>Now that you know how to generate audio outputs and send audio inputs, there are a few other techniques you might want to master.</p>
<ul>
    <li><a href="/docs/guides/text-to-speech">Text to speech</a> - Use a specialized model to turn text into speech.</li>
    <li><a href="/docs/guides/speech-to-text">Speech to text</a> - Use a specialized model to turn audio files with speech into text.</li>
    <li><a href="/docs/guides/realtime">Realtime API</a> - Learn to use the Realtime API to prompt a model over a WebSocket.</li>
    <li><a href="/docs/api-reference/chat">Full API reference</a> - Check out all the options for audio generation in the API reference.</li>
</ul>

<span style="font-family: Arial, sans-serif; margin-bottom: 10px;">Was this page useful?</span>
<button class="button">👍</button>
<button class="button">👎</button>


</body>
</html>
