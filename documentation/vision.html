<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision - OpenAI API</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f4f4f4;
        }
        h1, h2, h3 {
            color: #333;
        }
        code {
            background-color: #e7e7e7;
            padding: 5px;
            border-radius: 4px;
        }
        pre {
            background-color: #e7e7e7;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
        a {
            color: #007BFF;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        section {
            margin-bottom: 30px;
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
    </style>
</head>
<body>

    <h1>Vision</h1>
        <h2>Learn how to use vision capabilities to understand images.</h2>
        <p>Many OpenAI <a href="/docs/models">models</a> have vision capabilities, meaning the models can take in images and answer questions about them. Historically, language model systems have been limited by taking in a single input modality, text.</p>


        <h2>Quickstart</h2>
        <p>Images are made available to the model in two main ways: by passing a link to the image or by passing the base64 encoded image directly in the request. Images can be passed in the <code>user</code> messages.</p>

        <h3>Analyze the content of an image</h3>

        <label>JavaScript</label>
        <pre><code class="language-javascript">
import OpenAI from "openai";
const openai = new OpenAI();

const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "What's in this image?" },
        {
          type: "image_url",
          image_url: {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        },
      ],
    },
  ],
});

console.log(response.choices[0]);
        </code></pre>

        <label>python</label>
        <pre><code class="language-python">
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image?"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                    },
                },
            ],
        }
    ],
    max_tokens=300,
)

print(response.choices[0]);
        </code></pre>

        <label>curl</label>
        <pre><code class="language-bash">
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What is in this image?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
            }
          }
        ]
      }
    ],
    "max_tokens": 300
  }'
        </code></pre>

        <p>The model is best at answering general questions about what is present in the images. While it does understand the relationship between objects in images, it is not yet optimized to answer detailed questions about the location of certain objects in an image. For example, you can ask it what color a car is or what some ideas for dinner might be based on what is in your fridge, but if you show it an image of a room and ask it where the chair is, it may not answer the question correctly.</p>
        <p>It is important to keep in mind the <a href="/docs/guides/vision#limitations">limitations of the model</a> as you explore what use-cases visual understanding can be applied to.</p>

    <section>
        <h2>Uploading Base64 encoded images</h2>
        <p>If you have an image or set of images locally, you can pass those to the model in base 64 encoded format, here is an example of this in action:</p>
        <pre><code class="language-python">
import base64
from openai import OpenAI

client = OpenAI()

# Function to encode the image
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

# Path to your image
image_path = "path_to_your_image.jpg"

# Getting the base64 string
base64_image = encode_image(image_path)

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "What is in this image?",
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                },
            ],
        }
    ],
)

print(response.choices[0])
        </code></pre>
    </section>

    <section>
        <h2>Multiple image inputs</h2>
        <p>The Chat Completions API is capable of taking in and processing multiple image inputs in both base64 encoded format or as an image URL. The model will process each image and use the information from all of them to answer the question.</p>

        <pre><code class="language-javascript">
import OpenAI from "openai";
const openai = new OpenAI();

const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "What are in these images? Is there any difference between them?" },
        {
          type: "image_url",
          image_url: {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        },
        {
          type: "image_url",
          image_url: {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        }
      ],
    },
  ],
});
console.log(response.choices[0]);
        </code></pre>

        <pre><code class="language-python">
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "What are in these images? Is there any difference between them?",
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                    },
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                    },
                },
            ],
        }
    ],
    max_tokens=300,
)
print(response.choices[0])
        </code></pre>

        <pre><code class="language-bash">
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are in these images? Is there any difference between them?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            }
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            }
          }
        ]
      }
    ],
    "max_tokens": 300
  }'
        </code></pre>
    </section>

    <section>
        <h2>Low or high fidelity image understanding</h2>
        <p>By controlling the <code>detail</code> parameter, which has three options, <code>low</code>, <code>high</code>, or <code>auto</code>, you have control over how the model processes the image and generates its textual understanding. By default, the model will use the <code>auto</code> setting which will look at the image input size and decide if it should use the <code>low</code> or <code>high</code> setting.</p>
        
      
    
        <p><code>low</code> will enable the "low res" mode. The model will receive a low-res 512px x 512px version of the image, and represent the image with a budget of 85 tokens. This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail.</p>
        <p><code>high</code> will enable "high res" mode, which first allows the model to first see the low res image (using 85 tokens) and then creates detailed crops using 170 tokens for each 512px x 512px tile.</p>

        <h3>Choosing the detail level</h3>
        <pre><code class="language-javascript">
import OpenAI from "openai";
const openai = new OpenAI();

const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "What's in this image?" },
        {
          type: "image_url",
          image_url: {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            "detail": "low"
          },
        },
      ],
    },
  ],
});

console.log(response.choices[0]);
        </code></pre>

        <pre><code class="language-python">
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image?"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                        "detail": "high",
                    },
                },
            ],
        }
    ],
    max_tokens=300,
)

print(response.choices[0].message.content)
        </code></pre>

        <pre><code class="language-bash">
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What is in this image?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
              "detail": "high"
            }
          }
        ]
      }
    ],
    "max_tokens": 300
  }'
        </code></pre>
    </section>

    <section>
        <h2>Managing images</h2>
        <p>The Chat Completions API, unlike the Assistants API, is not stateful. That means you have to manage the messages (including images) you pass to the model yourself. If you want to pass the same image to the model multiple times, you will have to pass the image each time you make a request to the API.</p>
        <p>For long running conversations, we suggest passing images via URLs instead of base64. The latency of the model can also be improved by downsizing your images ahead of time to be less than the maximum size they are expected them to be. For low res mode, we expect a 512px x 512px image. For high res mode, the short side of the image should be less than 768px and the long side should be less than 2,000px.</p>
        <p>After an image has been processed by the model, it is deleted from OpenAI servers and not retained. <a href="https://openai.com/enterprise-privacy">We do not use data uploaded via the OpenAI API to train our models</a>.</p>
    </section>

    <section>
        <h2>Limitations</h2>
        <p>While GPT-4 with vision is powerful and can be used in many situations, it is important to understand the limitations of the model. Here are some of the limitations we are aware of:</p>
        <ul>
            <li>Medical images: The model is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.</li>
            <li>Non-English: The model may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean.</li>
            <li>Small text: Enlarge text within the image to improve readability, but avoid cropping important details.</li>
            <li>Rotation: The model may misinterpret rotated / upside-down text or images.</li>
            <li>Visual elements: The model may struggle to understand graphs or text where colors or styles like solid, dashed, or dotted lines vary.</li>
            <li>Spatial reasoning: The model struggles with tasks requiring precise spatial localization, such as identifying chess positions.</li>
            <li>Accuracy: The model may generate incorrect descriptions or captions in certain scenarios.</li>
            <li>Image shape: The model struggles with panoramic and fisheye images.</li>
            <li>Metadata and resizing: The model doesn't process original file names or metadata, and images are resized before analysis, affecting their original dimensions.</li>
            <li>Counting: May give approximate counts for objects in images.</li>
            <li>CAPTCHAS: For safety reasons, we have implemented a system to block the submission of CAPTCHAs.</li>
        </ul>
    </section>

    <section>
        <h2>Calculating costs</h2>
        <p>Image inputs are metered and charged in tokens, just as text inputs are. The token cost of a given image is determined by two factors: its size, and the <code>detail</code> option on each <code>image_url</code> block. All images with <code>detail: low</code> cost 85 tokens each. <code>detail: high</code> images are first scaled to fit within a 2048 x 2048 square, maintaining their aspect ratio. Then, they are scaled such that the shortest side of the image is 768px long. Finally, we count how many 512px squares the image consists of. Each of those squares costs <strong>170 tokens</strong>. Another <strong>85 tokens</strong> are always added to the final total.</p>
        <p>Here are some examples demonstrating the above:</p>
        <ul>
            <li>A 1024 x 1024 square image in <code>detail: high</code> mode costs 765 tokens.</li>
            <li>A 2048 x 4096 image in <code>detail: high</code> mode costs 1105 tokens.</li>
            <li>A 4096 x 8192 image in <code>detail: low</code> mode costs 85 tokens.</li>
        </ul>
    </section>

    <section>
        <h2>FAQ</h2>
        <h3>Can I fine-tune the image capabilities in <code>gpt-4</code>?</h3>
        <p>Vision fine-tuning is available for some models - <a href="/docs/guides/fine-tuning#vision">learn more</a>.</p>

        <h3>Can I use <code>gpt-4</code> to generate images?</h3>
        <p>No, you can use <code>dall-e-3</code> to generate images and <code>gpt-4o</code>, <code>gpt-4o-mini</code> or <code>gpt-4-turbo</code> to understand images.
            <h3>What type of files can I upload?</h3>
        <p>We currently support PNG (<code>.png</code>), JPEG (<code>.jpeg</code> and <code>.jpg</code>), WEBP (<code>.webp</code>), and non-animated GIF (<code>.gif</code>).</p>

        <h3>Is there a limit to the size of the image I can upload?</h3>
        <p>Yes, we restrict image uploads to 20MB per image.</p>

        <h3>Can I delete an image I uploaded?</h3>
        <p>No, we will delete the image for you automatically after it has been processed by the model.</p>

        <h3>Where can I learn more about the considerations of GPT-4 with Vision?</h3>
        <p>You can find details about our evaluations, preparation, and mitigation work in the <code>GPT-4 with Vision</code> system card.</p>

        <h3>How do rate limits for GPT-4 with Vision work?</h3>
        <p>We process images at the token level, so each image we process counts towards your tokens per minute (TPM) limit. See the calculating costs section for details on the formula used to determine token count per image.</p>

        <h3>Can GPT-4 with Vision understand image metadata?</h3>
        <p>No, the model does not receive image metadata.</p>

        <h3>What happens if my image is unclear?</h3>
        <p>If an image is ambiguous or unclear, the model will do its best to interpret it. However, the results may be less accurate. A good rule of thumb is that if an average human cannot see the info in an image at the resolutions used in low/high res mode, then the model cannot either.</p>
    </section>

    <span class="feedback-text">Was this page useful?</span>
    <button class="button">👍</button>
    <button class="button">👎</button> 